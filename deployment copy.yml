$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: current
endpoint_name: vllm-hf-2
environment_variables:
  MODEL_NAME: Noumaan/phi3-mini-128k-instruct-4bit-quantized # define the model name using the identifier from HG
  VLLM_ARGS: "--max-num-seqs 16 --enforce-eager" # optional args for vLLM runtime
  HUGGING_FACE_HUB_TOKEN: <hf token> # use this, if you want to authenticate to HF
environment:
  image: <image path> # paste Docker image address here
  inference_config:
    liveness_route:
      port: 8000
      path: /ping
    readiness_route:
      port: 8000
      path: /health
    scoring_route:
      port: 8000
      path: /
instance_type: Standard_D48a_v4
instance_count: 1
request_settings: # This section is optional, yet important for optimizing throughput
    max_concurrent_requests_per_instance: 1
    request_timeout_ms: 10000
liveness_probe:
  initial_delay: 10
  period: 10
  timeout: 2
  success_threshold: 1
  failure_threshold: 30
readiness_probe:
  initial_delay: 120 # wait for 120s before we start probing, so the model can load peacefully
  period: 10
  timeout: 2
  success_threshold: 1
  failure_threshold: 30
